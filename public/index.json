
[{"content":"","date":"2021-07-11","externalUrl":null,"permalink":"/life/","section":"Lives","summary":"","title":"Lives","type":"life"},{"content":"","date":"2021-03-28","externalUrl":null,"permalink":"/life/%E6%97%A0%E9%A2%98/","section":"Lives","summary":"","title":"无题","type":"life"},{"content":" 这世界\n大概就是这个样子的吧\n一样的夜晚，一样的歌\n一样的心情\n坠入这无边的夜\n等着世界将我唤醒\n把我的无知\n再次赋给我\n夜晚的人们，思绪往往比在白天更加多愁善感。19 年 3 月写下此诗。觉得后 3 句有些意境在其中。\n","date":"2021-03-28","externalUrl":null,"permalink":"/life/%E5%A4%9C/","section":"Lives","summary":"\u003cblockquote\u003e\n\u003cp\u003e这世界\u003c/p\u003e\n\u003cp\u003e大概就是这个样子的吧\u003c/p\u003e\n\u003cp\u003e一样的夜晚，一样的歌\u003c/p\u003e\n\u003cp\u003e一样的心情\u003c/p\u003e\n\u003cp\u003e坠入这无边的夜\u003c/p\u003e","title":"夜","type":"life"},{"content":" Python 中的\u0026quot;self\u0026quot;是什么 # 在使用 pycharm 编写 Python 时，自动补全总会把函数定义的第一个参数定义为 self 。遂查，总结如下：\nself 大体上和静态语言如 Java 中的 this 关键字类似，用于指代实例变量。只是在 Python 中需要主动定义在函数的参数中。但是通过实例调用方法时，无须传入 self 参数。 self 不是关键字，只是一种官方推荐写法，也可以写成其他的名称，但是很容易造成误解，所以不推荐。 由上方的知识可知：如果一个函数如果不是定义在类中，那么就不需要定义 self 变量了。即使定义了，也会作为一个普通的参数使用，参考下方的代码：\ndef outer_func(self, val2): print(self, \u0026#34;\\n\u0026#34; + val2) outer_func(\u0026#34;just a common parameter\u0026#34;, \u0026#34;another common parameter\u0026#34;) # 执行结果 # just a common parameter # another common parameter ","date":"2021-03-08","externalUrl":null,"permalink":"/tech/python-%E4%B8%AD%E7%9A%84-self-%E6%98%AF%E4%BB%80%E4%B9%88/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003ePython 中的\u0026quot;self\u0026quot;是什么 \n    \u003cdiv id=\"python-中的self是什么\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#python-%e4%b8%ad%e7%9a%84self%e6%98%af%e4%bb%80%e4%b9%88\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h1\u003e\n\u003cp\u003e在使用 \u003ccode\u003epycharm\u003c/code\u003e 编写 Python 时，自动补全总会把函数定义的第一个参数定义为 \u003ccode\u003eself\u003c/code\u003e 。遂查，总结如下：\u003c/p\u003e","title":"Python 中的「self」是什么？","type":"tech"},{"content":"","date":"2021-03-08","externalUrl":null,"permalink":"/tech/","section":"Teches","summary":"","title":"Teches","type":"tech"},{"content":"","date":"2021-03-08","externalUrl":null,"permalink":"/","section":"关于","summary":"","title":"关于","type":"page"},{"content":" 站在地下通道入口\n一眼就能看到尽头的地铁站\n我向着它走去\n就像走向我平庸的生活\n继续重蹈覆辙\n我看向人群\n他们平静如水面\n等待被微风吹皱\n或是被一块石子炸起\n那些支离破碎不会留下任何痕迹\n我也是其中一员\n我心知肚明\n2月份某天下班，于体育西路地下通道入口处，抬头一眼能望到地铁站入口。而我不得不顺着人潮走向它。有感，遂作此诗。\n","date":"2021-02-27","externalUrl":null,"permalink":"/life/%E5%9C%B0%E4%B8%8B%E9%80%9A%E9%81%93%E5%85%A5%E5%8F%A3%E5%A4%84%E9%9A%8F%E6%83%B3/","section":"Lives","summary":"\u003cblockquote\u003e\n\u003cp\u003e站在地下通道入口\u003c/p\u003e\n\u003cp\u003e一眼就能看到尽头的地铁站\u003c/p\u003e\n\u003cp\u003e我向着它走去\u003c/p\u003e\n\u003cp\u003e就像走向我平庸的生活\u003c/p\u003e","title":"地下通道入口处随想","type":"life"},{"content":" 什么是双重分派 # 什么是分派（dispatch） # 首先我们需要理解「分派」的含义。分派就是将方法调用与对应的具体方法绑定起来。而判断的依据有两点，这两者可称为「宗量」：\n方法的接收者，也就是哪个对象调用了这个方法 方法的参数，调用方法时传递过来的参数 而不同的判断方法也就产生了不同的分派类型：\n静态分派 V.S. 动态分派\n单重分派 V.S. 多重分派\n下面以我的理解简单地总结一下这几个名词的含义：\n静态分派：静态分派在编译期进行，只根据宗量的静态类型进行判断，在编译完成后不会发生改变。也可称为「编译期多态」。\n动态分派：动态分派在运行期进行，根据宗量的动态类型来判断需要调用哪个方法。也称「运行时多态」。\n多重分派：虽然说 multiple 是「多重」的意思，但是实际上也可称为「双重分派」。多重分派就是同时根据两种宗量的类型进行判断。\n单重分派：单重分派就是只根据其中一种宗量进行判断。\n所以其实这两组词是从两个不同的角度来描述分派的类型，并不冲突。\nJava 中的分派机制 # 在网上很多博客说 Java 中是单重分派；也有人说 Java 中实际上是静态双重分派，动态单重分派。下面是我的推论和总结。\nJava 中的重载机制，是静态分派，其根据方法传入参数的静态类型进行判断，而方法的接收者，也直接在编译期就确定。所以是静态的双重分派。\n而重写机制，肯定是动态分派了。但是其只根据方法接收者的动态类型进行判断。并不会根据方法参数的动态类型进行判断。所以是动态的单重分派。\n那如果一个方法同时被重写和重载了呢，那这个方法属于什么分派机制呢？我认为，重写和重载本来就分别是编译期和运行期的多态机制，就是分开进行讨论的，在程序不同生命周期时间段的分派机制直接分开讨论并进行定义就好了。\n访问者模式与伪双重分派 # 虽然说 Java 在编译期表现出了多重分派，但是也只能定性为单重分派语言，而这在设计代码时就会引起一些不便：在调用方法时无法根据参数的动态类型来分派对应的方法。所以我们只能曲线救国，可以通过以下两种方式实现伪动态双重分派。\ninstanceof # 在方法内部可以通过 Java 的 instanceof 关键字来判断对象的动态类型。通过分支选择来进行对应的处理操作。但是这种思路会让处理代码变得十分臃肿，而且违背了「开闭原则」。每次要新增一个被处理类时，就要修改代码新增一个分支进行处理。具体的实现方法很简单，不赘述。\n访问者模式 # 设计模式中的「访问者模式」就是通过利用伪双重分派来实现的。主要思路如下：\n设调用方为「类 A 」，被调用方为「类 B 」。通过反转调用方和被调用方，让类 A 重写方法来调用类 B 的重载方法。通过 this 变量将类 A 的动态类型传递给类 B 的重载方法，实现了伪双重分派。\n在下面这个例子中，我们想让 Player.play() 方法根据 Instrument 的动态类型来分别调用对应的重载方法，从而进行不同的处理：\nclass Player { void play(Instrument instrument) { System.out.println(\u0026#34;instrument notes\u0026#34;); } void play(Violin violin) { System.out.println(\u0026#34;player starts playing violin\u0026#34;); violin.note(); } void play(Piano piano) { System.out.println(\u0026#34;player starts playing piano\u0026#34;); piano.note(); } } class Instrument { void note() { System.out.println(\u0026#34;instrument notes\u0026#34;); } void accept(Player player) { player.play(this); } } class Violin extends Instrument { @Override public void note() { System.out.println(\u0026#34;violin notes\u0026#34;); } @Override public void accept(Player player) { player.play(this); } } class Piano extends Instrument { @Override public void note() { System.out.println(\u0026#34;piano notes\u0026#34;); } @Override public void accept(Player player) { player.play(this); } } public class Main { public static void main(String[] args) { Player player = new Player(); System.out.println(\u0026#34;-------双重分派-------\u0026#34;); Instrument[] instruments = {new Violin(), new Piano()}; for (Instrument instrument : instruments) { instrument.accept(player); } System.out.println(\u0026#34;-------单重分派-------\u0026#34;); for (Instrument instrument : instruments) { player.play(instrument); } } } // 结果 /* -------双重分派------- player starts playing violin violin notes player starts playing piano piano notes -------单重分派------- instrument notes instrument notes */ 从上方的代码运行结果可以得知，在调用重载方法时，不会根据对象的动态类型进行判断，而是根据静态类型判断，只会调用 play(Instrument) 方法。当然，我们在重载 Player 的方法时可以直接调用 Instrument.note() 方法，让编译器去调用子类重写后的方法。\n但是使用双重分派的意义在于， 把具体的处理逻辑放在调用者 Player 类的方法中，减少对 Instrument 的实现类的修改。事实上 Instrument 的子类只需要实现一个 accept(Player) 方法即可。\n访问者模式的出现基本上也就是因为 Java 这类语言的单重分派特性吧。如果是多重分派的语言，就可以直接利用语言特性了。\n参考文章：\n面向对象语言的多分派、单分派、双重分派 What is dispatching in JAVA? ","date":"2020-12-20","externalUrl":null,"permalink":"/tech/java%E4%B8%AD%E7%9A%84%E5%A4%9A%E9%87%8D%E5%88%86%E6%B4%BE%E4%B8%8E%E8%AE%BF%E9%97%AE%E8%80%85%E6%A8%A1%E5%BC%8F/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003e什么是双重分派 \n    \u003cdiv id=\"什么是双重分派\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%e4%bb%80%e4%b9%88%e6%98%af%e5%8f%8c%e9%87%8d%e5%88%86%e6%b4%be\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h1\u003e\n\n\u003ch2 class=\"relative group\"\u003e什么是分派（dispatch） \n    \u003cdiv id=\"什么是分派dispatch\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#%e4%bb%80%e4%b9%88%e6%98%af%e5%88%86%e6%b4%bedispatch\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\u003cp\u003e首先我们需要理解「分派」的含义。分派就是将方法调用与对应的具体方法绑定起来。而判断的依据有两点，这两者可称为「宗量」：\u003c/p\u003e","title":"Java 中的多重分派与访问者模式","type":"tech"},{"content":" 1. 概述 # 一个完整的采集链路的流程如下：\n所以要进行采集链路的部署需要以下几个步聚：\nnginx的配置 filebeat部署 logstash部署 kafka部署 kudu部署 下面将详细说明各个部分的部署方法，以及一些基本的配置参数。\n2. 部署流程 # nginx # 1. 安装 # nginx安装直接去官网下载一个压缩文件解压然后用sbin/nginx运行就可以了。\n2. 配置 # 2.1 日志输出格式 # nginx是采集链路的第一个环节，后面的日志采集系统是通过采集nginx日志进行分析的。本节主要对nginx的日志处理的配置进行描述。\n对nginx.conf文件进行配置：\nlog_format log_json escape=json \u0026#39;{ \u0026#34;@timestamp\u0026#34;: \u0026#34;$time_local\u0026#34;, \u0026#39; \u0026#39;\u0026#34;remote_addr\u0026#34;: \u0026#34;$remote_addr\u0026#34;, \u0026#39; \u0026#39;\u0026#34;referer\u0026#34;: \u0026#34;$http_referer\u0026#34;, \u0026#39; \u0026#39;\u0026#34;project\u0026#34;: \u0026#34;$arg_project\u0026#34;, \u0026#39; \u0026#39;\u0026#34;request\u0026#34;: \u0026#34;$request\u0026#34;, \u0026#39; \u0026#39;\u0026#34;status\u0026#34;: $status, \u0026#39; \u0026#39;\u0026#34;bytes\u0026#34;: $body_bytes_sent, \u0026#39; \u0026#39;\u0026#34;request_body\u0026#34;: \u0026#34;$request_body\u0026#34;,\u0026#39; \u0026#39;\u0026#34;data\u0026#34;: \u0026#34;$arg_data\u0026#34;,\u0026#39; \u0026#39;\u0026#34;cookies\u0026#34;: \u0026#34;$http_cookie\u0026#34;\u0026#39; \u0026#39; }\u0026#39;; 上面的代码定义了一个nginx的日志输出格式并命名为log_json，并且使用escape=json参数来把变量中可能包含的json字符串自动转义。\n这个日志输出哪些变量都可以灵活配置，取决于采集框架怎么进行数据解析。在本文中，前端发送的请求中，埋点数据可能出现在request body中，也可能出现在url参数中以data=xxx的形式转递，所以将这两个变量打印到日志中。\n输出格式也可以灵活配置，在logstash的输入解析中进行对应的调整就可以了。使用json格式可以直接用json插件进行解析，否则可能就要用grok插件自己写正则进行解析了。这里推荐输出为json格式。\n2.2 监听配置 # ... http { ... server { listen 2346; access_log logs/data_tracking/access.log log_json; location / { error_page 405 =200 $request_uri; } } } 配置监听埋点端口，将请求日志打印到单独的一个文件夹中避免和其他日志混淆，并指定输出格式为上一节配置好的log_json。\n同时把根请求的405代码转发为200，因为请求监听的端口并没有任何资源，请求会返回status_code = 405。\nfilebeat # filebeat是一款开源的轻量级日志采集工具，主要用于整合各个路径下的各种不同的日志文件，并且统一输出到指定的输出点。\n对于filebeat的配置较为简单，而filebeat提供的功能也十分有限，只能进行简单的日志采集工作，所以需要和logstash配和使用\n1. 安装 # 可以直接在官网下载rpm或deb包：\nhttps://www.elastic.co/cn/downloads/beats/filebeat\n下载后直接使用yum localinstall {pacakgeName}进行安装。\n或者使用包管理软件如 apt 和 yum 进行安装：\n首先引入公有签名key\nsudo rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch 然后再添加一个repo文件到/etc/yum.repos.d/目录中\n[elastic-7.x] name=Elastic repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md 最后使用yum命令直接安装即可。\n2. 配置和启动 # filebeat默认安装在/usr/share/filebeat下\nfilebeat目录结构说明：\nhome 安装的根目录 bin 一些二进制执行文件 config 配置文件 data 持久化的数据文件 logs filebeat运行日志 在运行filebeat前，需要对filebeat的一些运行参数进行配置。\n首先需要在filebeat根目录下新建一个filebeat.yml配置文件，并在其中写入如下的内容：\nfilebeat.inputs: - type: log enabled: true paths: - /var/log/*.log output.logstash: hosts:[\u0026#34;localhost:5044\u0026#34;] 注意：所有的横线后都要空一格，不能直接跟字符串。这是YAML的语法格式，可以去了解一下。\nfilebeat.inputs指定了输入的配置。其中type指定了输入的类型是日志类型。paths指定了输入文件的路径，表示读取/var/log/路径下的所有.log结尾的文件。\noutput.logstash指定了输出到logstash的配置。其中hosts可以指定一个数组，写入多个输出地址。\n设置开机启动filebeat\nsystemctl enable filebeat 通过bin/filebeat文件启动filebeat用:\nbin/filebeat -e -c filebeat.yml 3. 和nginx的配合 # 首先修改filebeat.yml中的paths，指定输入为nginx埋点日志的输出位置。\n使用include_lines，指定正则表达式来过滤不符合要求的行。\nnginx日志输出格式是一个请求一行。否则进行单行过滤的时候会有大问题。\n输出配置最好指定为内网ip。\nfilebeat: inputs: - type: log enabled: true paths: - /usr/local/openresty/nginx/logs/data_tracking/access.log* include_lines: [\u0026#39;\u0026#34;project\u0026#34;: \u0026#34;test_bank_event\u0026#34;\u0026#39;] output.logstash: hosts: [\u0026#34;192.168.0.34:5055\u0026#34;] logstash # logstash相对于filebeat并没有那么轻量，但相对来说有更多的功能和数据处理功能。所以一般将filebeat和logstash结合使用。用filebeat读取日志文件并输出到logstash中，再进行后续的数据处理。\n1. 安装 # logstash的安装方法和filebeat的安装基本相同，可以在官网下载安装包进行安装，在此不再赘述。\nhttps://www.elastic.co/cn/downloads/logstash\n2. 配置和启动 # logstash 的默认安装路径也位于/usr/share/目录下。\n其启动文件是安装根目录下的bin/logstash，对于一些简单的配置信息，可以直接以命令行参数的形式指定：\nbin/logstash -e \u0026#39;input { stdin {}} output{ stdout{}}\u0026#39; 这行命令指定了最基本的input和output为标准输入和标准输出。启动后直接在命令行输入内容，回车后就可以看到结构化处理后的数据输出：\nhello world /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/awesome_print-1.7.0/lib/awesome_print/formatters/base_formatter.rb:31: warning: constant ::Fixnum is deprecated { \u0026#34;@timestamp\u0026#34; =\u0026gt; 2020-06-08T03:38:04.658Z, \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;hadoop\u0026#34;, \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;hello world\u0026#34;, \u0026#34;@version\u0026#34; =\u0026gt; \u0026#34;1\u0026#34; } 要进行更多的配置，最好在安装根目录下新建一个 .conf格式的配置文件。\n要将logstash和filebeat联合使用，需要让logstash接受一个beats格式的输入：\ninput{ beats{ port =\u0026gt; \u0026#34;5044\u0026#34; } } # filter{ # # } output { stdout { codec =\u0026gt; rubydebug} } 上述配置指定了一个beats类型的input，并设置端口为5044\n在写好配置文件后可以使用下面的命令对配置进行检查，其中-f指定了要加载的配置文件：\nbin/logstash -f first-pipeline.conf --config.test_and_exit --config.test_and_exit命令会加载配置文件并检查是否有错误，并在输出检查结果后自动退出。\n如果检查通过，那么就可以启动logstash了：\nbin/logstash -f first-pipeline.conf --config.reload.automatic --config.reload.automatic命令开启了自动重载配置的功能，所以在修改完配置文件后可以不用重启服务，logstash会自动加载更新后的配置。\n如果在filebeat中配置的输出地址和logstash中的输入beats地址相同，那么在启动了这两个服务后，logstash就可以收到filebeat读取到的日志内容。其基本格式和上文中从标准输入获取数据并输出的格式类似。\n3. 和filebeat的配合 # 在输入配置了对应的filebeat的端口后，接下来主要是配置filter模块对日志进行解析，这部分也要根据nginx日志的结构进行灵活的配置。filebeat主要用来收集数据传输给logstash，当然filebeat中也可以进行一些简单的数据处理，但还是推荐filebeat只负责收集日志，发送到logstash进行统一处理。\n下面是一份logstash的conf文件用于参考：\nfilter{ # 首先对filebeat中的数据进行解析，其中\u0026#39;message\u0026#39;对应的值才是我们真正要解析的埋点数据 # 其他还有很多filebeat自动生成的数据。 # 这里使用json插件进行解析，解析完后就可以删除原来的变量了，减少不必要的数据传输 json{ source =\u0026gt; \u0026#34;message\u0026#34; remove_field =\u0026gt; [\u0026#34;message\u0026#34;] } # 对埋点数据进行url解码，因为埋点数据可能是通过url传输的，可能会有进行过url编码 urldecode{ all_fields =\u0026gt; true } # 使用ruby进行base64解码，目标是\u0026#39;data\u0026#39;变量，结果赋值给新建的\u0026#39;b64_decoded\u0026#39;变量。 ruby { init =\u0026gt; \u0026#34;require \u0026#39;base64\u0026#39;\u0026#34; code =\u0026gt; \u0026#34;event.set(\u0026#39;b64_decoded\u0026#39;, Base64.decode64(event.get(\u0026#39;data\u0026#39;))) if event.include?(\u0026#39;data\u0026#39;)\u0026#34; #这里解码出刚刚截取出来的msg字段 remove_field =\u0026gt; [\u0026#34;data\u0026#34;,\u0026#34;request\u0026#34;] } # 在解码后再进行一次json解析，把json字符串转换为json结构体 json { source =\u0026gt; \u0026#34;b64_decoded\u0026#34; remove_field =\u0026gt; [\u0026#34;b64_decoded\u0026#34;] } } 4. 和kafka的配合 # 由于本链路使用的存储为kudu数据库，而logstash是不支持直接写入kudu的；并且要考虑到并发处理和容错，所以要先将数据写入kafka消息队列，再由kafka写入kudu。\n主要是配置conf文件中的output模块：\noutput{ kafka { # 输出为json格式 codec =\u0026gt; json # 指定topic为\u0026#39;test_event\u0026#39; topic_id =\u0026gt; \u0026#34;test_event\u0026#34; } } 必要的配置就只有topic_id，更多的配置参考logstash官方文档对kafka插件的介绍。\n​\n","date":"2020-06-18","externalUrl":null,"permalink":"/tech/%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E9%93%BE%E8%B7%AF%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97/","section":"Teches","summary":"\u003ch1 class=\"relative group\"\u003e1. 概述 \n    \u003cdiv id=\"1-概述\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700\"\n            style=\"text-decoration-line: none !important;\" href=\"#1-%e6%a6%82%e8%bf%b0\" aria-label=\"锚点\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h1\u003e\n\u003cp\u003e一个完整的采集链路的流程如下：\u003c/p\u003e","title":"日志采集链路部署指南","type":"tech"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]