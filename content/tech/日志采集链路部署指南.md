---
title: "日志采集链路部署指南"
date: 2020-06-18T17:42:45+08:00
---



# 1. 概述

一个完整的采集链路的流程如下：

![image-20200907094954218](/img/image-20200907094954218.png)

所以要进行采集链路的部署需要以下几个步聚：

1. nginx的配置
2. filebeat部署
3. logstash部署
4. kafka部署
5. kudu部署

下面将详细说明各个部分的部署方法，以及一些基本的配置参数。

# 2. 部署流程

## nginx

### 1. 安装

nginx安装直接去官网下载一个压缩文件解压然后用`sbin/nginx`运行就可以了。

### 2. 配置

#### 2.1 日志输出格式

nginx是采集链路的第一个环节，后面的日志采集系统是通过采集nginx日志进行分析的。本节主要对nginx的日志处理的配置进行描述。

对`nginx.conf`文件进行配置：

```conf
 log_format log_json escape=json '{ "@timestamp": "$time_local", '
                        '"remote_addr": "$remote_addr", '
                        '"referer": "$http_referer", '
                        '"project": "$arg_project", '
                        '"request": "$request", '
                        '"status": $status, '
                        '"bytes": $body_bytes_sent, '
                        '"request_body": "$request_body",'
                        '"data": "$arg_data",'
                        '"cookies": "$http_cookie"'
                        ' }';

```

上面的代码定义了一个nginx的日志输出格式并命名为`log_json`，并且使用`escape=json`参数来把变量中可能包含的json字符串自动转义。

这个日志输出哪些变量都可以灵活配置，取决于采集框架怎么进行数据解析。在本文中，前端发送的请求中，埋点数据可能出现在request body中，也可能出现在url参数中以`data=xxx`的形式转递，所以将这两个变量打印到日志中。

输出格式也可以灵活配置，在logstash的输入解析中进行对应的调整就可以了。使用`json`格式可以直接用`json`插件进行解析，否则可能就要用`grok`插件自己写正则进行解析了。这里推荐输出为`json`格式。

#### 2.2 监听配置

```
 ...
 
 http {
 	...
 	
 	server {
         listen 2346;
         access_log logs/data_tracking/access.log log_json;
         location / {
             error_page 405 =200 $request_uri;
         }
     }
}
```

配置监听埋点端口，将请求日志打印到单独的一个文件夹中避免和其他日志混淆，并指定输出格式为上一节配置好的`log_json`。

同时把根请求的`405`代码转发为`200`，因为请求监听的端口并没有任何资源，请求会返回`status_code = 405`。



## filebeat

filebeat是一款开源的轻量级日志采集工具，主要用于整合各个路径下的各种不同的日志文件，并且统一输出到指定的输出点。

对于filebeat的配置较为简单，而filebeat提供的功能也十分有限，只能进行简单的日志采集工作，所以需要和logstash配和使用

### 1. 安装

可以直接在官网下载rpm或deb包：

https://www.elastic.co/cn/downloads/beats/filebeat

下载后直接使用`yum localinstall {pacakgeName}`进行安装。

或者使用包管理软件如 apt 和 yum 进行安装：

首先引入公有签名key

```shell
sudo rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
```

然后再添加一个repo文件到`/etc/yum.repos.d/`目录中

```shell
[elastic-7.x]
name=Elastic repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
```

最后使用`yum`命令直接安装即可。

### 2. 配置和启动

filebeat默认安装在`/usr/share/filebeat`下

filebeat目录结构说明：

- `home` 安装的根目录
- `bin` 一些二进制执行文件
- `config` 配置文件
- `data` 持久化的数据文件
- `logs` filebeat运行日志

在运行filebeat前，需要对filebeat的一些运行参数进行配置。

首先需要在filebeat根目录下新建一个`filebeat.yml`配置文件，并在其中写入如下的内容：

```yml
filebeat.inputs:
- type: log
  enabled: true
  paths: - /var/log/*.log
output.logstash:
  hosts:["localhost:5044"]
```

> 注意：所有的横线后都要空一格，不能直接跟字符串。这是YAML的语法格式，可以去了解一下。



`filebeat.inputs`指定了输入的配置。其中`type`指定了输入的类型是日志类型。`paths`指定了输入文件的路径，表示读取`/var/log/`路径下的所有`.log`结尾的文件。

`output.logstash`指定了输出到logstash的配置。其中`hosts`可以指定一个数组，写入多个输出地址。

设置开机启动filebeat

```bash
systemctl enable filebeat
```

通过`bin/filebeat`文件启动filebeat用:

```shell
bin/filebeat -e -c filebeat.yml
```

### 3. 和nginx的配合

1. 首先修改`filebeat.yml`中的`paths`，指定输入为nginx埋点日志的输出位置。

2. 使用`include_lines`，指定正则表达式来过滤不符合要求的行。

   > nginx日志输出格式是一个请求一行。否则进行单行过滤的时候会有大问题。

3. 输出配置最好指定为内网ip。

```yml
filebeat:
  inputs:
    - type: log
      enabled: true
      paths:
        - /usr/local/openresty/nginx/logs/data_tracking/access.log*
      include_lines: ['"project": "test_bank_event"']

output.logstash:
    hosts: ["192.168.0.34:5055"]
```





## logstash

logstash相对于filebeat并没有那么轻量，但相对来说有更多的功能和数据处理功能。所以一般将filebeat和logstash结合使用。用filebeat读取日志文件并输出到logstash中，再进行后续的数据处理。

### 1. 安装

logstash的安装方法和filebeat的安装基本相同，可以在官网下载安装包进行安装，在此不再赘述。

https://www.elastic.co/cn/downloads/logstash

### 2. 配置和启动

logstash 的默认安装路径也位于`/usr/share/`目录下。

其启动文件是安装根目录下的`bin/logstash`，对于一些简单的配置信息，可以直接以命令行参数的形式指定：

```shell
bin/logstash -e 'input { stdin {}} output{ stdout{}}'
```

这行命令指定了最基本的`input`和`output`为标准输入和标准输出。启动后直接在命令行输入内容，回车后就可以看到结构化处理后的数据输出：

```shell
hello world
/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/awesome_print-1.7.0/lib/awesome_print/formatters/base_formatter.rb:31: warning: constant ::Fixnum is deprecated
{
    "@timestamp" => 2020-06-08T03:38:04.658Z,
          "host" => "hadoop",
       "message" => "hello world",
      "@version" => "1"
}

```

要进行更多的配置，最好在安装根目录下新建一个 .conf格式的配置文件。

要将logstash和filebeat联合使用，需要让logstash接受一个beats格式的输入：

```yml
input{
	beats{
		port => "5044"
	}
}
# filter{
#
# }
output {
	stdout { codec => rubydebug}
}
```

上述配置指定了一个beats类型的input，并设置端口为5044

在写好配置文件后可以使用下面的命令对配置进行检查，其中`-f`指定了要加载的配置文件：

```shell
bin/logstash -f first-pipeline.conf --config.test_and_exit
```

`--config.test_and_exit`命令会加载配置文件并检查是否有错误，并在输出检查结果后自动退出。

如果检查通过，那么就可以启动logstash了：

```shell
bin/logstash -f first-pipeline.conf --config.reload.automatic
```

`--config.reload.automatic`命令开启了自动重载配置的功能，所以在修改完配置文件后可以不用重启服务，logstash会自动加载更新后的配置。

如果在filebeat中配置的输出地址和logstash中的输入beats地址相同，那么在启动了这两个服务后，logstash就可以收到filebeat读取到的日志内容。其基本格式和上文中从标准输入获取数据并输出的格式类似。

### 3. 和filebeat的配合

在输入配置了对应的filebeat的端口后，接下来主要是配置`filter`模块对日志进行解析，这部分也要根据`nginx`日志的结构进行灵活的配置。`filebeat`主要用来收集数据传输给`logstash`，当然`filebeat`中也可以进行一些简单的数据处理，但还是推荐`filebeat`只负责收集日志，发送到`logstash`进行统一处理。

下面是一份`logstash`的conf文件用于参考：

```ruby
filter{
    # 首先对filebeat中的数据进行解析，其中'message'对应的值才是我们真正要解析的埋点数据
    # 其他还有很多filebeat自动生成的数据。
    # 这里使用json插件进行解析，解析完后就可以删除原来的变量了，减少不必要的数据传输
	json{
        source => "message"
        remove_field => ["message"]
    }
    # 对埋点数据进行url解码，因为埋点数据可能是通过url传输的，可能会有进行过url编码
    urldecode{
        all_fields => true
    }
    # 使用ruby进行base64解码，目标是'data'变量，结果赋值给新建的'b64_decoded'变量。
    ruby {
        init => "require 'base64'"
        code => "event.set('b64_decoded', Base64.decode64(event.get('data'))) if event.include?('data')"
        #这里解码出刚刚截取出来的msg字段
        remove_field => ["data","request"]
    }
    # 在解码后再进行一次json解析，把json字符串转换为json结构体
    json {
        source => "b64_decoded"
        remove_field => ["b64_decoded"]
    }
}
```





### 4. 和kafka的配合

由于本链路使用的存储为`kudu`数据库，而logstash是不支持直接写入`kudu`的；并且要考虑到并发处理和容错，所以要先将数据写入`kafka`消息队列，再由`kafka`写入`kudu`。

主要是配置conf文件中的`output`模块：

```ruby
output{
    kafka {
        # 输出为json格式
        codec => json
        # 指定topic为'test_event'
        topic_id => "test_event"
    }
}
```

必要的配置就只有`topic_id`，更多的配置参考logstash官方文档对`kafka`插件的介绍。

​	